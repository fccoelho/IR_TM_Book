\chapter{Chapter}

    
    

    
    \section{Modelando assuntos}\label{modelando-assuntos}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{gensim} \PY{k}{import} \PY{n}{corpora}\PY{p}{,} \PY{n}{models}\PY{p}{,} \PY{n}{similarities}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{tokenize} \PY{k}{import} \PY{n}{WordPunctTokenizer}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{stopwords}
        \PY{k+kn}{from} \PY{n+nn}{string} \PY{k}{import} \PY{n}{punctuation}
        \PY{k+kn}{from} \PY{n+nn}{pprint} \PY{k}{import} \PY{n}{pprint}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Using gpu device 0: GeForce GT 640 (CNMeM is disabled, cuDNN 5004)

    \end{Verbatim}

    \subsection{Definindo um problema}\label{definindo-um-problema}

A epidemia de Zika gerou uma epidemia de publicações científicas sobre o
assunto. Como é uma doença pouco conhecida, pesquisas em diversas áreas
do conhecimento precisam ser conduzidas para poder preencher a lacuna da
nossa ignorância.

Neste exercício vamos analizar um corpus de resumos de artigos
publicados sobre o assunto e tentar modelar os assuntos existentes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{dicionario} \PY{o}{=} \PY{n}{corpora}\PY{o}{.}\PY{n}{Dictionary}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dicionario\PYZus{}zika.dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{corpus} \PY{o}{=} \PY{n}{corpora}\PY{o}{.}\PY{n}{MmCorpus}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{corpus\PYZus{}zika}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{dicionario}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}
        \PY{l+m+mi}{498}\PY{o}{*}\PY{l+m+mi}{5886}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Dictionary(5886 unique tokens: ['fattening', 'x894', 'Mercer', 'hosts', 'artery']{\ldots})
MmCorpus(498 documents, 5886 features, 24027 non-zero entries)

    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:} 2931228
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{dicionario}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{k}{for} \PY{n}{doc} \PY{o+ow}{in} \PY{n}{corpus}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{doc}\PY{p}{)}
             \PY{k}{break}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
virus
[(0, 2.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 3.0), (6, 1.0), (7, 2.0), (8, 3.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 2.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 2.0), (17, 1.0), (18, 2.0), (19, 1.0), (20, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 2.0), (26, 1.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 2.0), (31, 1.0), (32, 2.0), (33, 1.0), (34, 1.0), (35, 2.0), (36, 1.0), (37, 3.0), (38, 3.0), (39, 1.0), (40, 3.0), (41, 1.0), (42, 1.0), (43, 2.0), (44, 1.0), (45, 2.0), (46, 3.0), (47, 3.0), (48, 5.0), (49, 1.0), (50, 1.0), (51, 2.0), (52, 2.0), (53, 1.0), (54, 1.0), (55, 1.0), (56, 1.0), (57, 1.0), (58, 1.0), (59, 1.0), (60, 1.0), (61, 1.0), (62, 2.0), (63, 1.0), (64, 1.0), (65, 1.0), (66, 1.0), (67, 3.0), (68, 3.0), (69, 1.0), (70, 2.0), (71, 1.0), (72, 1.0), (73, 1.0), (74, 1.0), (75, 1.0), (76, 2.0), (77, 1.0), (78, 5.0), (79, 1.0), (80, 1.0), (81, 1.0), (82, 1.0), (83, 12.0), (84, 1.0), (85, 1.0), (86, 1.0), (87, 1.0), (88, 1.0), (89, 2.0), (90, 2.0), (91, 1.0), (92, 1.0), (93, 3.0), (94, 1.0), (95, 3.0), (96, 2.0), (97, 1.0), (98, 1.0), (99, 1.0), (100, 1.0), (101, 1.0), (102, 1.0), (103, 1.0), (104, 1.0), (105, 1.0), (106, 1.0), (107, 4.0), (108, 2.0), (109, 1.0), (110, 4.0), (111, 1.0), (112, 1.0), (113, 3.0), (114, 1.0), (115, 2.0), (116, 1.0), (117, 1.0), (118, 2.0), (119, 1.0), (120, 1.0), (121, 1.0), (122, 1.0), (123, 1.0), (124, 1.0), (125, 1.0), (126, 1.0), (127, 1.0), (128, 3.0), (129, 2.0), (130, 1.0), (131, 2.0), (132, 9.0), (133, 1.0), (134, 4.0), (135, 2.0), (136, 1.0), (137, 1.0), (138, 1.0), (139, 1.0), (140, 2.0), (141, 1.0), (142, 1.0), (143, 1.0), (144, 1.0), (145, 1.0), (146, 1.0), (147, 1.0), (148, 1.0), (149, 1.0), (150, 1.0), (151, 1.0), (152, 1.0), (153, 1.0), (154, 1.0), (155, 1.0), (156, 1.0), (157, 2.0), (158, 2.0)]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}

    \subsection{Latent Semantic Indexing -
LSI}\label{latent-semantic-indexing---lsi}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{tfidf} \PY{o}{=} \PY{n}{models}\PY{o}{.}\PY{n}{TfidfModel}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}
         \PY{n}{corpus\PYZus{}tfidf} \PY{o}{=} \PY{n}{tfidf}\PY{p}{[}\PY{n}{corpus}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{lsi} \PY{o}{=} \PY{n}{models}\PY{o}{.}\PY{n}{LsiModel}\PY{p}{(}\PY{n}{corpus\PYZus{}tfidf}\PY{p}{,} \PY{n}{id2word}\PY{o}{=}\PY{n}{dicionario}\PY{p}{,} \PY{n}{num\PYZus{}topics}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
         \PY{n}{corpus\PYZus{}lsi} \PY{o}{=} \PY{n}{lsi}\PY{p}{[}\PY{n}{corpus\PYZus{}tfidf}\PY{p}{]}
\end{Verbatim}

    Depois de estimar o modelo, podemos olhar os 30 assuntos, listando
apenas as 4 palavras mais importantes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{lsi}\PY{o}{.}\PY{n}{show\PYZus{}topics}\PY{p}{(}\PY{l+m+mi}{30}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} [(0, '0.342*"ZIKV" + 0.259*"virus" + 0.192*"Zika" + 0.151*"infection"'),
          (1, '-0.516*"ZIKV" + 0.242*"women" + 0.188*"virus" + 0.179*"pregnant"'),
          (2, '-0.337*"ZIKV" + 0.251*"YF" + 0.200*"antibodies" + 0.194*"antibody"'),
          (3, '-0.275*"ZIKV" + -0.171*"women" + 0.135*"spread" + -0.116*"pregnant"'),
          (4,
           '0.373*"Ae" + 0.177*"transmission" + -0.156*"microcephaly" + 0.141*"aegypti"'),
          (5,
           '-0.281*"YF" + 0.202*"patients" + -0.165*"microcephaly" + -0.149*"pregnancy"'),
          (6, '-0.250*"Ae" + 0.199*"ZIKV" + -0.172*"Brazil" + -0.157*"microcephaly"'),
          (7, '-0.396*"YF" + 0.129*"viruses" + 0.119*"blood" + 0.119*"public"'),
          (8, '-0.157*"sequence" + 0.146*"Ae" + -0.143*"sequences" + 0.136*"ZIKV"'),
          (9, '-0.387*"YF" + -0.165*"Ae" + 0.141*"viruses" + -0.130*"saliva"'),
          (10, '0.200*"Guillain" + 0.198*"BarrÃ" + 0.194*"syndrome" + 0.186*"©"'),
          (11, '0.204*"blood" + -0.180*"Ae" + -0.119*"infection" + -0.116*"returning"'),
          (12,
           '-0.249*"antibody" + -0.187*"enhancement" + -0.186*"heterologous" + 0.152*"yellow"'),
          (13, '-0.222*"YF" + 0.164*"antibody" + 0.137*"enhancement" + 0.126*"Health"'),
          (14, '0.249*"xa0" + 0.220*"Â" + 0.144*".(" + -0.143*"women"'),
          (15,
           '0.361*"rabbit" + 0.212*"rabbits" + -0.211*"blood" + -0.122*"transfusion"'),
          (16, '0.148*".\textbackslash{}'," + 0.124*"Thailand" + -0.124*"pregnant" + 0.119*"sexual"'),
          (17, '-0.205*"xa0" + -0.178*"Â" + 0.129*"rabbits" + -0.122*"women"'),
          (18, '-0.233*"cells" + 0.201*"rabbit" + -0.182*"brain" + -0.170*"human"'),
          (19, '-0.180*"rabbit" + -0.158*"blood" + 0.104*"patients" + 0.101*"DENV"'),
          (20, '-0.125*"rabbit" + -0.117*"antibody" + 0.117*"PCR" + -0.111*"report"'),
          (21, '0.172*"rabbit" + -0.143*"rabbits" + 0.131*"DENV" + -0.118*"disease"'),
          (22,
           '0.248*"rabbits" + -0.194*"rabbit" + -0.144*"fever" + 0.130*"fattening"'),
          (23, '0.153*"risk" + -0.140*"saliva" + 0.132*"sexual" + 0.117*"enhancement"'),
          (24,
           '0.185*"rabbits" + -0.157*"rabbit" + -0.109*"disease" + 0.107*"fattening"'),
          (25,
           '-0.205*"rabbits" + 0.156*"abnormalities" + 0.152*".\textbackslash{}'," + -0.122*"Thailand"'),
          (26,
           '0.148*"patients" + 0.109*"species" + 0.087*"transmitted" + -0.084*"assay"'),
          (27,
           '-0.156*"countries" + 0.121*"mosquitoes" + -0.112*"GBS" + 0.106*"dengue"'),
          (28,
           '0.139*"vaccine" + -0.118*"viruses" + -0.110*"rabbits" + 0.105*"development"'),
          (29, '0.166*"DENV" + -0.131*"GBS" + -0.098*"women" + 0.097*"virus"')]
\end{Verbatim}
        
    Podemos também olhar para os documentos do nosso corpus como uma
combinação linear dos assuntos

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k}{for} \PY{n}{doc}  \PY{o+ow}{in} \PY{n}{corpus\PYZus{}lsi}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{doc}\PY{p}{)}
             \PY{k}{break}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[(0, 0.29341359575061515), (1, -0.044658797891151791), (2, -0.056700593373754361), (3, -0.3351010693844646), (4, 0.015495065028631893), (5, -0.083301342437953452), (6, -0.023842338412340961), (7, 0.081378555864377089), (8, 0.05638766110989972), (9, -0.031680886986972183), (10, 0.072017301610918294), (11, 0.054245633991349809), (12, 0.076329205826558846), (13, -0.0070387571250865536), (14, -0.11017238855926909), (15, 0.030608301182679713), (16, -0.053418795588331985), (17, -0.1130915871885457), (18, 0.097459120095664917), (19, 0.018477422670524206), (20, -0.045325825065361286), (21, -0.013672952175308169), (22, -0.033956342373825485), (23, -0.0053491432979650005), (24, 0.083119597914713225), (25, 0.021412510661699521), (26, 0.063817988915913892), (27, -0.066603861567659711), (28, 0.022462657935527139), (29, -0.024702260455466089)]

    \end{Verbatim}

    Podemos calcular a similaridade por assunto de um documento com todos os
demais documentos do corpus.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{index} \PY{o}{=} \PY{n}{similarities}\PY{o}{.}\PY{n}{MatrixSimilarity}\PY{p}{(}\PY{n}{corpus\PYZus{}lsi}\PY{p}{)}
\end{Verbatim}

    Vamos escolher o primeiro documento do corpus para ser a referência

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{sims} \PY{o}{=} \PY{n}{index}\PY{p}{[}\PY{n}{doc}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}pprint(list(enumerate(sims)))}
         \PY{n}{pprint}\PY{p}{(}\PY{n+nb}{sorted}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sims}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{reverse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[(0, 1.0),
 (430, 0.71284831),
 (12, 0.61172044),
 (494, 0.59754914),
 (11, 0.58494759),
 (6, 0.57779515),
 (401, 0.57517916),
 (477, 0.54245442),
 (27, 0.53723323),
 (14, 0.52353585)]

    \end{Verbatim}

    \subsection{Latent Dirichlet Allocation -
LDA}\label{latent-dirichlet-allocation---lda}

O LDA é uma técnica um pouco mais sofisticada que o LSI, que envolve uma
interpretação probabilística do que é uma assunto. Para saber mais, veja
este artigo: http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{lda\PYZus{}model} \PY{o}{=} \PY{n}{models}\PY{o}{.}\PY{n}{ldamodel}\PY{o}{.}\PY{n}{LdaModel}\PY{p}{(}\PY{n}{corpus}\PY{p}{,} \PY{n}{id2word}\PY{o}{=}\PY{n}{dicionario}\PY{p}{,} \PY{n}{num\PYZus{}topics}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{passes}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{lda\PYZus{}model}\PY{o}{.}\PY{n}{show\PYZus{}topics}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} [(8,
           "0.019*virus + 0.013*Zika + 0.011*The + 0.010*antibody + 0.009*cases + 0.009*viruses + 0.008*sera + 0.008*infection + 0.008*.'] + 0.008*['"),
          (19,
           '0.045*virus + 0.036*Zika + 0.020*transmission + 0.013*infection + 0.012*women + 0.010*pregnant + 0.009*health + 0.008*disease + 0.008*pregnancy + 0.006*ZIKV'),
          (17,
           '0.039*ZIKV + 0.013*brain + 0.012*), + 0.010*fetal + 0.010*microcephaly + 0.009*malaria + 0.009*RNA + 0.008*infections + 0.008*The + 0.008*gestation'),
          (28,
           '0.015*infection + 0.011*enhancement + 0.011*antibody + 0.009*showed + 0.008*dengue + 0.008*Africa + 0.008*The + 0.008*virus + 0.007*produced + 0.007*enhancing'),
          (5,
           '0.036*rabbit + 0.011*gene + 0.011*blood + 0.010*weight + 0.008*05 + 0.008*P + 0.008*genotype + 0.008*Zika + 0.007*0 + 0.007*ZIKAV'),
          (4,
           '0.017*virus + 0.013*viral + 0.013*nyong + 0.011*fever + 0.011*The + 0.008*area + 0.007*transmission + 0.006*areas + 0.006*exanthems + 0.006*infections'),
          (15,
           "0.032*virus + 0.017*Zika + 0.008*.'] + 0.008*Aedes + 0.008*[' + 0.008*human + 0.008*), + 0.007*The + 0.007*ZIKV + 0.006*fever"),
          (9,
           '0.012*LTI + 0.011*viruses + 0.009*virus + 0.009*obtained + 0.007*Zika + 0.007*mice + 0.007*vitro + 0.007*two + 0.007*five + 0.005*systems'),
          (6,
           '0.027*ZIKV + 0.021*Ae + 0.015*virus + 0.012*aegypti + 0.011*The + 0.010*mosquito + 0.009*mosquitoes + 0.009*vector + 0.008*Aedes + 0.008*dengue'),
          (23,
           '0.044*ZIKV + 0.012*virus + 0.011*Zika + 0.011*infection + 0.008*cells + 0.007*The + 0.006*Aedes + 0.006*viral + 0.005*Africa + 0.005*West')]
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} lsi.save\PY{o}{?}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    