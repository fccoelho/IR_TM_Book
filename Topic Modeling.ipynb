{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling topics in document collections\n",
    "\n",
    "Here we will explore some methods to make inference about the topics contained in document collections. Topics will be represented as probability distribution on the words belonging to the entire vocabulary of the corpus. Different topics will contain different proportions of words. Documents can also be described as mixtures of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a corpus\n",
    "\n",
    "The Zika pandemic in 2015-16 has generated a epidemic of scientific publications about it. Being an almost unknown disease at the time the pandemic started, the boom in publications was necessary to \"fill in the blanks\" of the knowledge about the Zika virus, the disease it caused and other related topics.\n",
    "\n",
    "Can we find out which were the most researched topics about Zika?\n",
    "\n",
    "In the follwoing exercise, we will analise a corpus of publication abstracts captured from the Pubmed database. We will be using the gensim package for topic modeling and to facilitate things, the corpus and the dictionary are available in the our software repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 640 (CNMeM is disabled, cuDNN 5004)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicionario = corpora.Dictionary.load('Dicionario_zika.dict')\n",
    "corpus = corpora.MmCorpus('corpus_zika')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(7901 unique tokens: ['azido', 'prevalent', 'manuscripts', ').', 'subcutaneous']...)\n",
      "MmCorpus(872 documents, 7901 features, 40533 non-zero entries)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2931228"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dicionario)\n",
    "print(corpus)\n",
    "498*5886"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the corpus consists of 872 documents, with 7901 unique words in its vocabulary. For Gensim, a document is a list of 2-tuples (word id, frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 5.0), (5, 1.0), (6, 2.0), (7, 1.0), (8, 1.0), (9, 2.0), (10, 2.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 3.0), (15, 2.0), (16, 3.0), (17, 2.0), (18, 1.0), (19, 1.0), (20, 2.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 2.0), (31, 1.0), (32, 1.0), (33, 9.0), (34, 1.0), (35, 2.0), (36, 1.0), (37, 1.0), (38, 1.0), (39, 1.0), (40, 1.0), (41, 1.0), (42, 1.0), (43, 4.0), (44, 1.0), (45, 2.0), (46, 1.0), (47, 3.0), (48, 2.0), (49, 1.0), (50, 1.0), (51, 1.0), (52, 1.0), (53, 1.0), (54, 1.0), (55, 1.0), (56, 1.0), (57, 1.0), (58, 1.0), (59, 2.0), (60, 1.0), (61, 1.0), (62, 1.0), (63, 5.0), (64, 1.0), (65, 1.0), (66, 1.0), (67, 1.0), (68, 1.0), (69, 2.0), (70, 1.0), (71, 1.0), (72, 2.0), (73, 1.0), (74, 1.0), (75, 1.0), (76, 2.0), (77, 1.0), (78, 3.0), (79, 3.0), (80, 1.0), (81, 3.0), (82, 2.0), (83, 2.0), (84, 1.0), (85, 1.0), (86, 1.0), (87, 2.0), (88, 1.0), (89, 2.0), (90, 1.0), (91, 1.0), (92, 1.0), (93, 2.0), (94, 3.0), (95, 1.0), (96, 1.0), (97, 1.0), (98, 1.0), (99, 1.0), (100, 2.0), (101, 1.0), (102, 3.0), (103, 4.0), (104, 1.0), (105, 1.0), (106, 3.0), (107, 2.0), (108, 1.0), (109, 1.0), (110, 1.0), (111, 1.0), (112, 1.0), (113, 3.0), (114, 1.0), (115, 1.0), (116, 1.0), (117, 1.0), (118, 2.0), (119, 1.0), (120, 1.0), (121, 1.0), (122, 1.0), (123, 1.0), (124, 1.0), (125, 1.0), (126, 2.0), (127, 2.0), (128, 1.0), (129, 3.0), (130, 2.0), (131, 1.0), (132, 1.0), (133, 1.0), (134, 1.0), (135, 1.0), (136, 1.0), (137, 1.0), (138, 1.0), (139, 3.0), (140, 1.0), (141, 1.0), (142, 1.0), (143, 1.0), (144, 2.0), (145, 1.0), (146, 1.0), (147, 1.0), (148, 1.0), (149, 1.0), (150, 2.0), (151, 1.0), (152, 1.0), (153, 4.0), (154, 3.0), (155, 1.0), (156, 12.0), (157, 2.0), (158, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing - LSI\n",
    "The first method we will use to model the topics in the corpus, is the LSI method, which stands for Latent Semantic embedding. Instead of the raw frequency of the words let's use the TF-IDF value of each word in each document, as a measure of the importance of a word to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In LSI we have to specify a priori, the number of topics we believe exist in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dicionario, num_topics=30)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After estimating the LSI model, we can inspect the subjects extracted. looking only at the 4 most important words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.344*\"ZIKV\" + 0.258*\"virus\" + 0.198*\"Zika\" + 0.166*\"infection\"'),\n",
       " (1, '0.611*\"ZIKV\" + -0.210*\"women\" + -0.205*\"Zika\" + -0.200*\"virus\"'),\n",
       " (2, '0.196*\"ZIKV\" + -0.187*\"fever\" + -0.174*\"YF\" + 0.170*\"microcephaly\"'),\n",
       " (3, '-0.202*\"ZIKV\" + 0.135*\"Aedes\" + 0.134*\"spread\" + -0.123*\"brain\"'),\n",
       " (4, '-0.234*\"Ae\" + -0.171*\"women\" + 0.168*\"cases\" + 0.167*\"patients\"'),\n",
       " (5, '-0.213*\"microcephaly\" + 0.201*\"ZIKV\" + -0.197*\"YF\" + -0.144*\"brain\"'),\n",
       " (6,\n",
       "  '-0.294*\"Ae\" + -0.146*\"albopictus\" + -0.144*\"aegypti\" + -0.137*\"infants\"'),\n",
       " (7, '0.246*\"ZIKV\" + -0.171*\"genome\" + -0.158*\"virus\" + 0.158*\"YF\"'),\n",
       " (8, '0.163*\"isolated\" + -0.161*\"patients\" + 0.156*\"YF\" + 0.154*\"strains\"'),\n",
       " (9, '-0.172*\"fever\" + 0.163*\"Health\" + -0.138*\"abnormalities\" + -0.123*\"CT\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics(10,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, within this paradigm, a document can be seen as a mixture, or linear combination of topics. In the object **corpus_lsi** we generate above, we can find this representation of the documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2832771940757487), (1, 0.034778412621503649), (2, 0.047663271800901949), (3, -0.3137186239666353), (4, -0.056148534274674929), (5, 0.05323903374508384), (6, -0.067096554390004878), (7, 0.11428521434543858), (8, -0.029369647498402856), (9, -0.014759474293403185), (10, 0.051569908114626209), (11, 0.044254355341419319), (12, -0.05691898773473978), (13, -0.068825112058023843), (14, -0.02195590821510561), (15, 0.0086818775684834159), (16, -0.04779029324525709), (17, 0.070210663187842887), (18, 0.081583964932671157), (19, 0.038797052128154964), (20, 0.073282907466170458), (21, -0.055100211956329297), (22, -0.028602713996770793), (23, -0.12490527006562716), (24, -0.037722543376292328), (25, -0.048239216200329033), (26, -0.013040690616555268), (27, -0.0086285804910014377), (28, -0.012187627915945793), (29, -0.010538037465571987)]\n"
     ]
    }
   ],
   "source": [
    "for doc  in corpus_lsi:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each document can be seen as a vector in a topic space. Thus we can calculate the cosine similarity between documents using this fact. To do that, we first calculate the matrix with all the similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(corpus_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's print the ten most similar documents to document $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0),\n",
      " (449, 0.68444359),\n",
      " (399, 0.66428459),\n",
      " (2, 0.61082482),\n",
      " (548, 0.61057496),\n",
      " (12, 0.60451663),\n",
      " (489, 0.57837856),\n",
      " (16, 0.57104939),\n",
      " (465, 0.56657535),\n",
      " (704, 0.56532925)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[doc]\n",
    "#pprint(list(enumerate(sims)))\n",
    "pprint(sorted(list(enumerate(sims)), key=lambda x:x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - LDA\n",
    "LDA is a similar technique to LSI but which models documents as a probability distribution of topics and topics as a probability distribution of words. Thus the weights are always positive and add to 1. To know more about LDA read this article: http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus, id2word=dicionario, num_topics=30, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, '0.014*The + 0.014*virus + 0.013*Zika + 0.011*infection'),\n",
       " (4, '0.016*ZIKV + 0.016*virus + 0.011*Zika + 0.010*),'),\n",
       " (5, '0.039*virus + 0.023*Zika + 0.012*The + 0.009*infection'),\n",
       " (20, '0.038*Zika + 0.037*virus + 0.025*women + 0.023*transmission'),\n",
       " (19, '0.027*Zika + 0.026*virus + 0.011*2 + 0.009*1'),\n",
       " (15, '0.025*virus + 0.021*Zika + 0.012*transmission + 0.009*ZIKV'),\n",
       " (2, '0.025*ZIKV + 0.021*virus + 0.010*Ae + 0.010*),'),\n",
       " (7, '0.017*virus + 0.014*ZIKV + 0.011*Zika + 0.011*fever'),\n",
       " (12, '0.023*x80 + 0.012*insecticide + 0.012*x89Â + 0.012*±'),\n",
       " (18, '0.015*ZIKV + 0.014*Zika + 0.012*infection + 0.009*human')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before documents are probability distributions over the set of 30 topics specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.095488416923020167),\n",
       " (4, 0.86107272694775772),\n",
       " (22, 0.039027091423336144)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lda = corpus_lda[3]\n",
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995588235294114"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t[1] for t in doc_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
